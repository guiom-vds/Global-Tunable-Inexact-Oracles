{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages' import\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.linalg import eigs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory requires the assumption of domain separability in most of the practical cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 : Penalty method for inverse problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q$ and $U$ are euclidean balls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-separable components:  $f(x) = min_{u \\in U}\\, F(x) + \\mathcal{F}(u) + \\frac{\\lambda}{2}||T_x\\,x + T_u\\,u- t||^2$\n",
    "\n",
    "$t$ is $d_x$ dimensional\n",
    "\n",
    "$T_x \\in \\mathbb{R}^{d_x\\times d_x}$\n",
    "\n",
    "$T_u \\in \\mathbb{R}^{d_x \\times d_u}$ might be $\\mathbf{0}$\n",
    "\n",
    "$F \\in \\mathcal{S}^{1,1}_{\\mu_x,L_x}(Q)$\n",
    "\n",
    "$\\mathcal{F} \\in \\mathcal{S}^{1,1}_{\\mu_u,L_u}(U)$\n",
    "\n",
    "case (1): bad aux, small dimension\n",
    "\n",
    "case (2): good aux, big dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimensional set-up\n",
    "pre_prop = 0.5\n",
    "pre_d = 10\n",
    "dx = int(np.ceil(pre_prop*pre_d)+2)\n",
    "du = int(pre_d-dx+2)\n",
    "d = dx+du\n",
    "\n",
    "## size of domains \n",
    "advantage = 1 # neutral at 1\n",
    "neutral = 1 # neutral at 1\n",
    "H_Q = neutral*dx*advantage\n",
    "H_U = neutral*du\n",
    "H_base = np.sqrt(H_Q**2 + H_U**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic functions\n",
    "\n",
    "# construction of T_x (invertible required)\n",
    "pre_pre_T_x = np.random.uniform(0,0.5,(dx,dx))\n",
    "pre_T_x = pre_pre_T_x.T@pre_pre_T_x\n",
    "T_x = pre_T_x + 0.001*np.eye(dx)\n",
    "biggest_x = np.real(eigs(T_x.T@T_x,1)[0][0])\n",
    "smallest_x = np.real(eigs(T_x.T@T_x,dx)[0][dx-1]) # at comment if dx too big and inherent lines below\n",
    "\n",
    "# construction of T_u\n",
    "T_u = np.random.uniform(0,1,(dx,du))\n",
    "biggest_u = np.real(eigs(T_u.T@T_u,1)[0][0])\n",
    "smallest_u = np.real(eigs(T_u.T@T_u,du)[0][du-1])\n",
    "\n",
    "# construction of T\n",
    "T = np.block([[T_x.T@T_x,T_x.T@T_u],[T_u.T@T_x,T_u.T@T_u]])\n",
    "\n",
    "# factice choice of minimizer\n",
    "t = np.random.normal(0,1,dx)\n",
    "u_opt = np.random.uniform(-5,5,du)\n",
    "x_opt = np.linalg.solve(T_x,t-T_u@u_opt)\n",
    "x_opt_no_Tu = np.linalg.solve(T_x,t)\n",
    "\n",
    "# biggest eigenvalue of sym' matrix, computed efficiently \n",
    "biggest = np.real(eigs(T,1)[0][0])\n",
    "smallest = np.real(eigs(T,d)[0][d-1])\n",
    "\n",
    "# lambda param for regularization (lambda should grow until a very large value but)\n",
    "lambda_param = 5/biggest\n",
    "\n",
    "# construction of F\n",
    "mu_F = 0.005\n",
    "L_F = 1\n",
    "mu_x_practice = mu_F\n",
    "L_x_practice = L_F + lambda_param*biggest_x\n",
    "\n",
    "# construction of F_bis\n",
    "mu_F_bis = 0.005\n",
    "L_F_bis = 1\n",
    "mu_u_practice = mu_F_bis\n",
    "L_u_practice = L_F_bis + lambda_param*biggest_u\n",
    "\n",
    "L_base_F_F_bis = np.max([L_F,L_F_bis])\n",
    "L_base_practice = L_base_F_F_bis+lambda_param*biggest\n",
    "mu_base_F_F_bis = np.min([mu_F,mu_F_bis])\n",
    "mu_base_practice = mu_base_F_F_bis\n",
    "mu_base_practice_advantageous = mu_base_F_F_bis+lambda_param*smallest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d_p = 1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## oracle cost model (FGD used to solve inner problem)\n",
    "oracle_K,oracle_beta,oracle_r = dx,du*H_U*np.sqrt(2*L_u_practice),1/2\n",
    "oracle_cost = lambda eta: oracle_K + oracle_beta/eta**oracle_r\n",
    "oracle_np_K,oracle_np_beta,oracle_np_r = d,0,1\n",
    "oracle_np_cost = lambda eta: oracle_np_K + oracle_np_beta/eta**oracle_np_r\n",
    "\n",
    "## delta cost model\n",
    "delta_h,delta_s = 1/2 * np.sqrt(8*H_Q**2*L_u_practice), 1/2\n",
    "delta_cost = lambda eta: delta_h/delta_s * eta**delta_s\n",
    "delta_np_h,delta_np_s = 0,1\n",
    "delta_np_cost = lambda eta: delta_np_h/delta_np_s * eta**delta_np_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 4]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [2,1,4]\n",
    "t[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## solve (I)\n",
    "\n",
    "N = 500\n",
    "\n",
    "def solve_I_instance1(epsilon_target,N,kappa,L,R,beta,r,h,s,la,lb,mode='GD'):\n",
    "\n",
    "    if mode=='FGD':\n",
    "        if kappa>0:\n",
    "            v = 1/(1+np.sqrt(kappa))\n",
    "            convergence_term = L*R**2/2 * v**(N-1)\n",
    "            q = v**(N-1-np.arange(N))\n",
    "        else:\n",
    "            convergence_term = 2*L*R**2 / N**2 # approx\n",
    "            q = (np.arange(N)+1)**2 / N**2\n",
    "    else:\n",
    "        if kappa>0:\n",
    "            v = (1-kappa)\n",
    "            convergence_term = np.min([L*R**2/2 * v**(N) / ((1-v**(N))/kappa), L*R**2/2 / N])\n",
    "            q = ((1-v**(N))/kappa)*v**(N-1-np.arange(N))\n",
    "        \n",
    "    Gamma_N_eps = epsilon_target - convergence_term\n",
    "    \n",
    "    delta_cost = lambda eta: h/s * eta**s\n",
    "    Phi = lambda Na,Nb: np.sum(delta_cost(lb)*q[:Nb]) + np.sum(delta_cost(la)*q[N+1-Na:]) \n",
    "    Psi = lambda Na,Nb: np.sum(h/s*(r*beta/h)**(s/(r+s))*q[Nb:N-Na]**(r/(r+s)))\n",
    "    bar_lambda = ((Gamma_N_eps - Phi(Na,Nb))/Psi(Na,Nb))**(-(r+s)/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## solve (II)\n",
    "\n",
    "N = 500\n",
    "\n",
    "budget_limit = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### greedy for complex sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : Stackelberg in game theory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q$ will be the simplex set whereas $U$ will denote a norm-ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 : Progressive gradient in large scale deterministic optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
